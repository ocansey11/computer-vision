{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ocansey11/computer-vision/blob/main/Final_Draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- <div style=\"text-align: center; margin: 0 auto; max-width: 800px;\">\n",
        "\n",
        "  <img src=\"https://raw.githubusercontent.com/gerryfrank10/ComputerVision/main/assignment/brand.gif\" alt=\"Bournemouth University\" style=\"width: 500px; margin: 0 auto; display: block;\">\n",
        "\n",
        "  <h1 style=\"text-align: center;\">Department of Computing & Informatics </h1> -->\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/gerryfrank10/ComputerVision/main/assignment/Bournemouth-University.jpg\" alt=\"Bournemouth University\" width=\"300\", height=\"300\">\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "  <strong>Department of Computing & Informatics </strong>\n",
        "</p>\n",
        "\n",
        "<!-- <hr style=\"max-width: 800px; margin: 20px auto;\"> -->\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "  <p align=\"center\"><b>Project Title </b>: Knapsack Problem Using Computer Vision</h1>\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "  <p align=\"center\">\n",
        "  <strong>Students:</strong><br>\n",
        "  Kevin Ocansey &nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp; Gerald Amasi &nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp; James Foster\n",
        "</p>\n",
        "\n",
        "<br />\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "<p align=\"center\">\n",
        "  <strong>Course:</strong> MSc Data Science & Artificial Intelligence<br>\n",
        "  <strong>Supervisor:</strong> Marthin Budhka<br>\n",
        "  <strong>Academic Year:</strong> 2024/2025\n",
        "</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "<hr style=\"max-width: 800px; margin: 20px auto;\">"
      ],
      "metadata": {
        "id": "WAk4yIiQiU24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract\n",
        "In this project, we present a method for efficiently packing a variety of items into a container with specific constraints. We utilise computer vision models, including YOLOv8, ResNet50, and OpenCV. By leveraging object detection and image processing techniques, we can measure the dimensions of everyday objects. We use YOLOv8 and Faster R-CNN based on the COCO dataset to draw bounding boxes. And using OpenCV to calibrate our findings. Combining these methods with an optimisation algorithm, we investigate ways to determine the size of objects from images and identify optimal arrangements for placing these items within a defined container that has known limitations.\n"
      ],
      "metadata": {
        "id": "CVkuIq2cddGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "The efficient use of space is a universal logistical challenge. Whether in shipping containers, warehouse shelving, delivery vans, or even digital storage, the need to fit multiple entities into limited space optimally is ever-present. This packing problem becomes significantly more complex when items vary in shape and size. With the rapid advancement of artificial intelligence and computer vision technologies, the possibility of automating this process has attracted growing attention. For example, Hanif et al. (2020) explored the application of neural network-based solvers for knapsack-like problems, demonstrating how even non-state-of-the-art methods can offer valuable alternatives for tackling such optimization challenges. Similarly, Franco et al. (2016) investigated how humans approach discrete optimization tasks like the knapsack problem, highlighting how problem complexity is perceived and solved differently in cognitive science versus computer science.\n",
        "\n",
        "<br />\n",
        "This type of spatial optimization is applicable across a wide range of domains from efficiently filling shopping baskets in supermarkets to warehouse logistics and industrial inventory management. Our project addresses a specific variation of this challenge: using a top-down camera to detect a set of items on a surface and determine whether they can fit into a predefined container. If placement is feasible, the system then estimates an optimal arrangement. The intended impact of this approach is to support automation efforts in settings such as e-commerce warehouses, moving logistics, and industrial manufacturing operations.\n"
      ],
      "metadata": {
        "id": "JpY06VtIdfip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem Definition\n",
        "\n",
        "\n",
        "### Rationale\n",
        "\n",
        "With the large growth of online shopping over the last decade, logistics has increased in global importance. More efficient packing leads to fewer shipments that are underutilising space, and so saving both space and time in packing scenarios is important. Packing items manually takes time and is prone to errors, leading to extra delays or non-optimal packing solutions.\n",
        "\n",
        "### Aims and objectives\n",
        "\n",
        "Our problem can be broken down into two primary objectives:\n",
        "\n",
        "* Feasibility Evaluation: Can the given set of detected items be arranged to fit within a designated container?\n",
        "* Optimization: If yes, what is the best way to arrange them to minimize wasted space, reduce the number of containers, or improve another metric of interest?\n",
        "\n",
        "\n",
        "We approach this through a layered strategy. First, we process visual data using computer vision to identify and extract item characteristics, primarily their two-dimensional dimensions (width and height). Then, we pass this information to an algorithm that attempts to virtually place the items into a container.\n"
      ],
      "metadata": {
        "id": "1XV4-WaGdkLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This packing challenge is closely related to variations of the ***knapsack and bin-packing problems*** in combinatorial optimization. The classic knapsack problem focuses on selecting items to maximize value within a weight constraint, and most versions consider 3d Taking3D—taking into account weight, height, and length. The 2D bin-packing problem, by contrast, focuses only on item dimensions and spatial arrangement.\n",
        "In addition, existing solutions for these problems typically assign a value to each item being placed in the bin or knapsack. The goal is to choose the best combination of items to maximize total value. We’ve slightly changed the objective in our problem to focus on optimizing space.\n",
        "What makes our approach different is that we don’t assign any value to the items, allitems—all items are treated equally. For our problem, we only care about whether the items fit and how efficiently we can arrange them. So instead of maximizing value, our goal is to minimize the space used within the container."
      ],
      "metadata": {
        "id": "J2kAr8iOdmxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data acquisition and preparation"
      ],
      "metadata": {
        "id": "c7KhKfWBl-0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Characteristics and Statistics\n",
        "\n",
        "#### Data Collection (Manually)\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/gerryfrank10/ComputerVision/main/assignment/datasets/im1-ref1.JPG\" alt=\"Multiple Bounding Boxes\" width=\"300\", height=\"300\">\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "  <strong>Figure : </strong> Example Image Collected Manually\n",
        "</p>\n",
        "\n",
        "We collected real-world images of various objects using a mobile camera. These images reflect natural conditions, including different lighting, angles and occultation. The objects vary in shape, and texture, mimicking realistic scenarios in which the system must identify and pack items efficiently\n",
        "For the collected images, over an iteration of testing and validating models detection performance, we propose an approach to use in capturing the images which include\n",
        "- Plain Background surface: The objects were placed on a plain surface with varying solid colors. This minimizes the background noise and enhances object boundary clarity, improving detection accuracy\n",
        "- Camera position and orientation: We also used consistent camera position to reduce the perspective distortion and improve object size estimation, the camera was mounted at a fixed height and angle. This consistency ensures uniformity in the dataset and aids in training the model effectively\n",
        "- Controlled lighting Conditions: Adequate lightning was used to avoid harsh shadows and reflections. When possible natural daylight was applied\n",
        "- Multiple angles and rotations: While maintaining controlled conditions, we captured each object from different angles and slight rotations to expose the model to minor variations in orientation and shape perception\n",
        "\n",
        "<hr />\n",
        "\n",
        "#### Open Image Dataset\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://storage.googleapis.com/openimages/web/images/crowdsource_dataset_3x3_example_a.png\" alt=\"Image Dataset\" width=\"300\", height=\"300\">\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <strong>Figure : </strong> Example image from the Open Images Dataset.\n",
        "</p>\n",
        "\n",
        "The data used for this project is derived from the Open Images dataset, which is a comprehensive resource for object detection, segmentation, and classification. Specifically, the subset we are working with contains bounding boxes, object segmentations, visual relationships, and localized narratives for 600 different object classes. This dataset spans across:\n",
        "\n",
        "- 1,743,042 training images,\n",
        "\n",
        "- 41,620 validation images,\n",
        "\n",
        "- 125,436 test images.\n",
        "\n",
        "We can apply filtering to isolate images that contain our target classes. This can come in handy to ensure retraining and specification for particular items we want for different context. The Dataset included *downloader.py* file which can help in downloading the images.\n",
        "```bash\n",
        "usage: downloader.py [-h] [--num_processes NUM_PROCESSES]\n",
        "                     [--download_folder DOWNLOAD_FOLDER]\n",
        "                     image_list\n",
        "\n",
        "Open Images image downloader.\n",
        "\n",
        "This script downloads a subset of Open Images images, given a list of image ids.\n",
        "Typical uses of this tool might be downloading images:\n",
        "- That contain a certain category.\n",
        "- That have been annotated with certain types of annotations (e.g. Localized\n",
        "Narratives, Exhaustively annotated people, etc.)\n",
        "\n",
        "The input file IMAGE_LIST should be a text file containing one image per line\n",
        "with the format <SPLIT>/<IMAGE_ID>, where <SPLIT> is either \"train\", \"test\",\n",
        "\"validation\", or \"challenge2018\"; and <IMAGE_ID> is the image ID that uniquely\n",
        "identifies the image in Open Images. A sample file could be:\n",
        "  train/f9e0434389a1d4dd\n",
        "  train/1a007563ebc18664\n",
        "  test/ea8bfd4e765304db\n",
        "\n",
        "positional arguments:\n",
        "  image_list            Filename that contains the split + image IDs of the\n",
        "                        images to download. Check the document\n",
        "\n",
        "options:\n",
        "  -h, --help            show this help message and exit\n",
        "  --num_processes NUM_PROCESSES\n",
        "                        Number of parallel processes to use (default is 5).\n",
        "  --download_folder DOWNLOAD_FOLDER\n",
        "                        Folder where to download the images.\n",
        "                      \n",
        "```\n",
        "\n",
        "For example packing items for a trip. Making our custom model lightweight. So for our given scenario lets say we focused on packing these three items (\"Spoon\", \"Plates\", \"Book\"), which helped us narrow down the dataset to a total of 10,330 images.\n",
        "\n",
        "\n",
        "\n",
        "### Data Processing\n",
        "Once the images are filtered to only include the target classes, the following preprocessing steps can be applied.\n",
        "\n",
        "\n",
        "* Bounding Box Information: The dataset provides bounding box coordinates for each object in an image. We'll extract the coordinates for each target class to ensure proper identification and scaling for later processing in the packing algorithm.\n",
        "\n",
        "* Image Resizing: To standardize the images and make them compatible with the algorithm, we may need to resize the images to a consistent dimension. This helps in reducing computational load and ensuring uniformity across the dataset.\n",
        "\n",
        "* Data Augmentation: To increase the diversity of the dataset and improve the robustness of the model, we may apply augmentation techniques like rotation, flipping, and cropping. This will simulate various real-world scenarios for item placement and packing.\n",
        "\n",
        "* Feature Extraction: We will extract key features like the item’s width, height, and aspect ratio from the bounding boxes. This is essential for determining if the items can fit within a specified container.\n",
        "\n",
        "* Splitting Data: The dataset will be split into training, validation, and test sets. This is critical for training the model, tuning hyperparameters, and evaluating the model's performance."
      ],
      "metadata": {
        "id": "SuCPxkufdomE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Visualization\n",
        "To help in understanding the dataset, visualizing some of the images and their corresponding bounding boxes can help verify that the correct items are being captured. A few steps to visualize the data might include.\n",
        "\n",
        "We used a function to display images which plots single image or multiple image\n"
      ],
      "metadata": {
        "id": "mEFcBHY0dr1_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW01tkTiclaU"
      },
      "outputs": [],
      "source": [
        "import numpy as np; import matplotlib.pyplot as plt; import cv2\n",
        "def show_img(im, ax=None, figsize=(8,8), title=None):\n",
        "    if not ax: _,ax = plt.subplots(1,1,figsize=figsize)\n",
        "    if len(im.shape)==2: im = np.tile(im[:,:,None], 3)\n",
        "    ax.imshow(im);\n",
        "    ax.xaxis.set_visible(False)\n",
        "    ax.yaxis.set_visible(False)\n",
        "    if title: ax.set_title(title)\n",
        "    return ax\n",
        "def show_imgs(ims, rows=1, figsize=(16,8), title=[None]):\n",
        "    title = title*len(ims) if len(title) == 1 else title\n",
        "    _,ax = plt.subplots(rows, len(ims)//rows, figsize=figsize)\n",
        "    [show_img(im,ax_,title=tit) for im,ax_,tit in zip(ims,ax.flatten(),title)]\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of Classes:\n",
        "\n",
        "Visualizing the distribution of bounding boxes for the target classes (e.g., how many bounding boxes correspond to plates vs. spoons vs. books)."
      ],
      "metadata": {
        "id": "DvKfNbVH-7eO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling : Why YOLOv8, ResNet50, and OpenCV?\n",
        "\n",
        "### YOLOv8: Precision in Object Detection\n",
        "\n",
        "**YOLOv8** (You Only Look Once version 8) is a state-of-the-art, real-time object detection model known for its speed and accuracy. Its architecture is optimized for detecting multiple objects within an image, making it particularly suitable for scenarios where numerous items need to be identified and localized simultaneously.\n",
        "\n",
        "- **Real-Time Processing**: YOLOv8 processes images in real-time, enabling immediate feedback on object detection, which is crucial for dynamic environments.\n",
        "- **High Accuracy**: It achieves a high mean Average Precision (mAP), ensuring reliable detection of objects, even in cluttered scenes.\n",
        "- **Versatility**: The model is adept at handling various object sizes and orientations, a common challenge in packing problems.\n",
        "\n",
        "These attributes make YOLOv8 an excellent choice for detecting and localizing items in a packing scenario, providing the foundational data needed for subsequent optimization steps.\n",
        "\n",
        "### ResNet50: Robust Feature Extraction\n",
        "\n",
        "**ResNet50** is a deep convolutional neural network that utilizes residual learning to facilitate the training of very deep networks. Its architecture includes 50 layers with skip connections, allowing for the effective learning of complex features without the risk of vanishing gradients.\n",
        "\n",
        "- **Deep Feature Learning**: The residual blocks enable the network to learn intricate patterns and features, which are essential for understanding the spatial relationships between objects.\n",
        "- **Transfer Learning**: Pretrained on large datasets like ImageNet, ResNet50 can be fine-tuned for specific tasks, reducing the need for extensive training data and computational resources.\n",
        "- **Versatility**: Its robust feature extraction capabilities make it suitable for various applications, including object detection and image classification.\n",
        "\n",
        "In the context of the 2D knapsack problem, ResNet50 can be employed to extract detailed features from the detected objects, aiding in the accurate assessment of their dimensions and spatial properties.\n",
        "\n",
        "###  OpenCV: Essential Preprocessing for Image Data\n",
        "\n",
        "**OpenCV** (Open Source Computer Vision Library) is a powerful tool for computer vision tasks, providing a wide range of utilities for image processing. When applied to the packing problem, OpenCV can significantly enhance the quality of input data before it is fed into more complex models like YOLOv8 and ResNet50.\n",
        "\n",
        "\n",
        "\n",
        "In order to increase the performance of the various models we suggest the following we will work with such as Fast R-cnn, Yolov8 we suggest the follwoing\n",
        "\n",
        "1. Minimize the problem domain: As we have already done. Limiting the images we will train on to four items. That is making use of the model to few search spaces (Intentionally over training to make it lightweight). Example if our context is we are concerned about filling books and DVD’s in the library, then we use a  pre-trained model with a dataset containing these particular images to make it efficient to determine the objects with high confidence.\n",
        "\n",
        "2. Performing Data Augmentation, with the dataset we collected, tailored to domain specific we perform data augmentation techniques to the dataset, so that we can get to have the model with base knowledge of items,"
      ],
      "metadata": {
        "id": "NVCqSgvedux8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing Appropriate Techniques\n",
        "\n",
        "We approached the 2D packing problem by integrating object detection, vision-based measurement correction, and mathematical optimization into a streamlined and automated pipeline. The system begins with the assumption that all items and the container are captured in a single top down image, where objects are unobstructed, rectangular, and aligned with the axes.\n",
        "\n",
        "#### a) Object Detection (Faster R-CNN / YOLOv8)\n",
        "\n",
        "To detect and localize the items within the input image, we used object detection models such as YOLOv8 and Faster R-CNN. These models, trained on datasets like COCO, were either used directly or fine tuned for our custom classes when needed. Each model outputs bounding boxes for detected items, with coordinates representing their pixel based height and width. This step provides the initial structural information required for spatial reasoning and optimization. The simplicity of our visual scenes, high contrast, fixed camera, and minimal clutter allowed us to achieve consistent detection results in most cases.\n",
        "\n",
        "#### b) Dimension Verification (OpenCV)\n",
        "\n",
        "Although deep learning models provide bounding boxes, we incorporated a verification and calibration step using OpenCV to enhance measurement reliability. Using `cv2.findContours` and `cv2.contourArea`, we were able to detect edges and isolate object outlines, filtering out noise and irrelevant detections. By placing a known size reference object (e.g., a coin or printed square) in each frame, we converted all pixel based measurements into real world units. This calibration process ensures that every item’s size is consistent with the scale of the container and suitable for accurate optimization.\n",
        "\n",
        "#### c) Optimization Model (Pyomo)\n",
        "\n",
        "After obtaining reliable width and height values for each item, we formulated the packing problem using Pyomo as a mixed integer linear program (MILP). The container was discretized into a grid, and each item was mapped to a corresponding number of grid cells based on its real-world dimensions. The optimization variables defined where each item could be placed and whether it should be rotated by 90 degrees. Constraints were imposed to prevent overlapping placements and to keep all items within the container boundaries. The optimization objective aimed to minimize unused space, effectively maximizing packing efficiency. This model serves as the decision-making core of the system.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "pIKDxrXFfFOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Well-Organized Approach\n",
        "\n",
        "The overall system was designed as a modular pipeline with clearly defined stages. Images are first passed through the detection module to extract item bounding boxes. These are then refined through OpenCV-based measurement correction before being passed to the optimization module. Each stage can be tested and improved independently, supporting iterative development and debugging.\n",
        "\n",
        "Validation of the pipeline was done using a synthetic dataset of rectangular items placed on known backgrounds. We created ground-truth packing scenarios and compared them to the system’s predicted arrangements, assessing correctness visually and quantitatively. This setup also allowed benchmarking of solver speed and placement accuracy under different configurations.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Y4zdZiWZyI3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Subsequent Steps Informed by Earlier Findings\n",
        "\n",
        "Early experimentation revealed that object detectors occasionally failed to produce accurate or complete bounding boxes. To mitigate this, we introduced an OpenCV-based fallback mechanism for contour verification. We also implemented a pre-optimization feasibility check by comparing the total area of detected items against the container area to eliminate clearly unsolvable cases.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P6E6YO3tyOTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Multiple Bounding Boxes](https://raw.githubusercontent.com/ocansey11/computer-vision/main/subsequent%20steps/multiple%20bounding%20boxes.png)\n"
      ],
      "metadata": {
        "id": "tciCD5zM1vCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, we improved detection quality by raising the score threshold used\n",
        "during the filtering phase of the fasterrcnn model. By only retaining predictions above a confidence level of 0.8, we reduced false positives and ensured that only high-certainty detections were passed to the optimization stage.\n",
        "\n",
        "```python\n",
        "\n",
        "    detections = []\n",
        "    for i, box in enumerate(predictions[0]['boxes']):\n",
        "        label_idx = predictions[0]['labels'][i].item()\n",
        "        label = coco_labels[label_idx]\n",
        "        # Make a condition to increase filtering of bounding boxes\n",
        "        if label in labels_of_interest and predictions[0]['scores'][i].item() > 0.8:\n",
        "            detections.append({\n",
        "                'label': label,\n",
        "                'bbox': box.tolist(),\n",
        "                'score': predictions[0]['scores'][i].item()\n",
        "            })\n",
        "\n",
        "    return detections, image_path\n",
        "  \n"
      ],
      "metadata": {
        "id": "EhlZbIBU2QQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Correct Bounding](https://raw.githubusercontent.com/ocansey11/computer-vision/main/subsequent%20steps/correct%20bounding.png)\n"
      ],
      "metadata": {
        "id": "ilSkCMWVyQw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenCv being our fall back we also observed some limitations with it. Mostly due to our setup. For example changing the background and lighting conditions significantly affected object detection accuracy with OpenCv.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VHzbhBrn4EOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![OpenCV Image](https://github.com/ocansey11/computer-vision/blob/main/subsequent%20steps/opencv%20img.jpeg?raw=true)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dZuAGZjZ4n5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we used a white background and diffused lighting, the results improved — contours were more distinct, and the bounding boxes were better aligned with object edges.\n",
        "\n",
        "However, even with these improvements, some objects were still not detected, especially when their color or texture closely matched the background.\n",
        "\n",
        "This led to inconsistent bounding boxes, often cutting off parts of the objects or leaving excess space. In other words, the accuracy of object detection felt a bit like luck.\n",
        "\n",
        "These limitations pushed us to consider more robust solutions. While OpenCV provides helpful preprocessing tools, we realized that relying solely on manual setups isn't sustainable. That’s why we started exploring deep learning-based models for more consistent and scalable object detection"
      ],
      "metadata": {
        "id": "4Kx3Ez81RSWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Preprocessing steps](https://github.com/ocansey11/computer-vision/blob/main/subsequent%20steps/opencv%20sucess.png?raw=true)"
      ],
      "metadata": {
        "id": "uQF9rWFcM_zX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenges in Grid-Based and Optimization-Driven Packing\n",
        "\n",
        "During the early phases of our project, we adopted a randomized grid based strategy for item placement. Items were inserted into the discretized container by randomly assigning positions based on width and height constraints, and we evaluated each trial by the amount of empty space left. While this brute-force strategy provided a useful visual baseline, it was quickly found to be computationally expensive and suboptimal, especially when item counts increased.\n",
        "\n",
        "A major realization was that naïvely placing all items at once failed to account for spatial dependencies. As a result, we revised our strategy to align items sequentially, one at a time, prioritizing space efficiency rather than uniform placement randomness. This shift marked a significant philosophical and computational change: the goal was no longer to fit all items, but rather to fit as many as possible, introducing selectivity and prioritization into the packing pipeline.\n",
        "\n",
        "In parallel, we explored mathematical optimization using Pyomo, an open source modeling language for expressing optimization problems in Python. However, this revealed its own limitations. When the model could not find a feasible solution particularly under strict constraints or small grid unit sizes—the solver often failed to return partial placements. Instead, it rejected the configuration altogether, resulting in zero item placements (GLPK Solver output, see Fig. 2). This exposed a critical drawback of relying exclusively on traditional Integer Linear Programming (ILP) for flexible, real-world packing scenarios (Martello and Toth, 1990; Silva et al., 2021).\n",
        "\n",
        "### Refinements and Key Improvements\n",
        "These challenges catalyzed several innovations in our pipeline. Our early manual simulations drawing items on grid paper proved invaluable. By simulating rotation edge cases and boundary violations manually, we identified latent constraints that needed to be explicitly encoded. This visual-spatial reasoning helped shape our constraint logic, particularly for overlap avoidance and rotation feasibility.\n",
        "\n",
        "A particularly impactful refinement was the grid unit parameter. Initially hardcoded, we later exposed it as a tunable configuration, discovering its direct impact on solver complexity. Small units (e.g., 10) created millions of binary decision variables, overwhelming solvers. Coarser units (e.g., 30 or 50) drastically improved performance with minor accuracy trade offs. This insight turned grid resolution into an optimization lever in its own right a balance between computational feasibility and spatial precision.\n",
        "\n",
        "Additionally, the insight that ILP solvers may not always return best-effort solutions led us to consider more adaptive algorithms like Genetic Algorithms (GAs). These heuristic methods offer flexibility by evolving placement strategies over generations and are robust under looser constraints, even if they sacrifice theoretical optimality (Mitchell, 1998). Though still under development, GA variants are positioned as viable alternatives for real time or large-scale packing tasks.\n"
      ],
      "metadata": {
        "id": "-Jv_57er2Hqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmarking and Dataset Generation\n",
        "In order to evaluate the robustness and scalability of our packing algorithms, we grounded our experiments on well-established benchmark datasets. A key dataset used during early benchmarking was the one introduced by Elhedhli, Gzara, and Yildiz (2019). Their dataset is known for representing realistic three-dimensional bin packing problems, particularly in contexts like mixed-case palletization, and includes realistic volume distributions, load-bearing constraints, and stacking logic.\n",
        "\n",
        "Since our problem domain was focused on 2D packing, we projected the 3D item dimensions from the Elhedhli dataset onto a 2D plane, treating each item as a flat rectangle defined by its width and depth. This projection was done while preserving relative scale and proportions, ensuring that our 2D simulations reflected meaningful geometric diversity.\n",
        "\n",
        "However, as our experiments progressed, we needed a way to generate new item configurations flexibly — with control over parameters like dimension ranges, weight distributions, affinities, and incompatibilities. To achieve this, we integrated the Q4RealBPP DataGen tool developed by TECNALIA (2023), an open-source Python-based generator designed for bin packing research and simulation.\n",
        "\n",
        "Using this tool, we created a new synthetic dataset of rectangular items by replicating the statistical distributions of dimensions found in Elhedhli et al.’s work. The generator leverages log-normal and normal distributions to simulate realistic variation in item sizes, aspect ratios, and volume categories. We configured parameters such as:\n",
        "\n",
        "Number of categories and items (e.g., 15 categories, 50 total items),\n",
        "\n",
        "Ranges for width, height, and depth (e.g., 0–1000 units),\n",
        "\n",
        "Weight range per item (20–50 units),\n",
        "\n",
        "Optional features like positive affinities and incompatibilities between items.\n",
        "\n",
        "This allowed us to produce large, varied batches of packing instances for stress-testing both ILP and heuristic solvers under controlled but realistic conditions.\n",
        "\n",
        "Our generated dataset was output in .txt format for solver ingestion, and all data were stored in Pandas-compatible formats for reproducibility and auditability. Importantly, we designed the generation process to be repeatable, ensuring consistent experimental conditions across solver configurations\n"
      ],
      "metadata": {
        "id": "Y0v1DJPMW6J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### 4. Automation (Avoiding Manual Steps)\n",
        "\n",
        "From start to finish, the pipeline is designed for full automation. Once an image is placed in the input folder, the system processes it through detection, dimension correction, optimization, and visualization without requiring human intervention. All measurements are extracted, scaled, and passed to our different algorithms to provide a structured placements. The placements contain\n",
        "\n",
        "The final arrangement is rendered on the original image or a blank grid, showing the optimized layout for review. In batch mode, the system can process an entire dataset of item and container images, making it practical for deployment in warehouse environments or packing line simulations.\n",
        "\n",
        "While our current implementation focuses on 2D, axis-aligned, rigid items, the underlying architecture supports future extensions. These could include 3D stacking, deformable object modelling, or even real-time decision-making via reinforcement learning.\n",
        "\n",
        "By structuring the pipeline this way, we ensure it is not only effective for the problem at hand but also extensible and robust to evolving constraints.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DiCn7dxxyRKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subsequent steps informed by earlier finding\n",
        "\n",
        "\n",
        "\n",
        "### Limitations of this approach\n",
        "\n",
        "For this approach there are few limitations including\n",
        "* There should be a reference object in the image, this is impractical solution in real world scenario where having a reference object can be hard to obtain in different scenarios\n",
        "* Items in the image space have to be aligned together, an example is taking an image of a person facing the sun, making the person bigger than the sun but in reality, the sun is bigger than the person. The reality is that having objects closer to each other and the position of the camera is an important factor to estimate  the size of objects. This is one of the factors that is beyond the scope of our project."
      ],
      "metadata": {
        "id": "Xrtagq3clb6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation:\n",
        "\n"
      ],
      "metadata": {
        "id": "BHx0sMgxmXXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison of Object Detection Models against baseline(s)\n",
        "\n",
        "| Model Name        | Average Estimation |\n",
        "|------------------------|--------------------|\n",
        "| CV2 - Model         | 1.2              |\n",
        "| YoloV8          | 2.5              |\n",
        "| YoloV11           | 4.0              |\n",
        "| Fast R-CNN         | 2.0              |\n",
        "\n",
        "\n",
        "With the comparison of both models included in the summary, (_ Model ) shows high performance compared to other models. This project concludes that the (_ Model) with proper calibration can be used to improve the model estimation performance.\n",
        "\n",
        "\n",
        "Comparison between Our Models with Practical Apple IOS Model\n",
        "\n",
        "* The models we selected are best in detecting multiple objects in image simultaneously, compared to the IOS Model which detects and determines the shape of objects one at a time\n",
        "* Using a pretrained model like fast-RCNN trained on COCO dataset,  we can detect the labels of our images and annotate them effectively, this implies a practical instance in Knapsack problems that involve importance of the object meaning the important object to be packed. The IOS model doesn’t include the labels of detected shapes in image, But this if only in the measurement Application, We believe this functionality can be added to their existing model to annotate the labels to the images.\n",
        "\n",
        "So for faster, efficient, and proper determination with other characteristics it's better to use a pretrained model, and train it to the dataset or data properties you would like to include.\n",
        "\n",
        "###Visualisations\n",
        "### Error analysis\n",
        "### Discussion and achievement of objective"
      ],
      "metadata": {
        "id": "aYdDOmkqmdA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of Packing Algorithm against baseline"
      ],
      "metadata": {
        "id": "GUUGvq7eySLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reproducibility\n"
      ],
      "metadata": {
        "id": "GiFP9WD9m1hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random;import torch\n",
        "def set_seeds(seed_value=42):\n",
        "  \"\"\"Set seed for reproducibility.\"\"\"\n",
        "  random.seed(seed_value)\n",
        "  torch.manual_seed(seed_value)\n",
        "  torch.cuda.manual_seed_all(seed_value)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seeds()\n"
      ],
      "metadata": {
        "id": "oO2sVb0hm3Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix:\n",
        "\n"
      ],
      "metadata": {
        "id": "F6bnlxxFmnBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this section we are going to include all the coding pipeline in this project. Start By installing dependencies"
      ],
      "metadata": {
        "id": "2gRGFjhpCVEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the packages"
      ],
      "metadata": {
        "id": "91sCDy1eDpAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt-get install -y glpk-utils\n",
        "!pip install torchvision torch opencv-python matplotlib numpy pandas pyomo boto3 requests ultralytics"
      ],
      "metadata": {
        "id": "bnw1dLjbCqh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all necessary libraries"
      ],
      "metadata": {
        "id": "8ijB2B0jDjFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "import pathlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import pyomo\n",
        "from pyomo.environ import SolverFactory\n",
        "from pyomo.environ import *\n",
        "import pyomo.environ as pyo\n",
        "import importlib.util\n",
        "import cv2\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from matplotlib import patches\n",
        "import torchvision.transforms as T\n",
        "import time\n",
        "from ultralytics import YOLO\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import math\n",
        "import importlib.util\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "GGYznoVUDPPz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "494fa20b-fefa-432e-83e9-0ea56a0bfa3f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7aeb8a853831>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m \u001b[0;31m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2108\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunctional\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2109\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_add_docstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m from torch.nn import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_parallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_gather\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscatter_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGather\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mScatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "include downloader for Open Image Dataset"
      ],
      "metadata": {
        "id": "RmpgA83CEYrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('downloader.py', 'wb') as f:\n",
        "    f.write(requests.get('https://raw.githubusercontent.com/openimages/dataset/master/downloader.py').content)"
      ],
      "metadata": {
        "id": "JolXtawoDtZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloading datasets"
      ],
      "metadata": {
        "id": "zpqQQYhBEUp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This code depends if you cant find the dataset for any reason in the file then download this dataset.\n",
        "'''\n",
        "url = 'https://github.com/gerryfrank10/ComputerVision/raw/refs/heads/main/assignment/datasets/images.zip'\n",
        "!wget {url}\n",
        "!unzip images.zip"
      ],
      "metadata": {
        "id": "n49Sa1r5EEAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load class descriptions and get the label name for selected classes\n",
        "classes = pd.read_csv(\n",
        "    'https://storage.googleapis.com/openimages/2018_04/class-descriptions-boxable.csv',\n",
        "    header=None, names=[\"LabelID\", \"LabelName\"]\n",
        ")\n",
        "target_classes = [\"Plate\", \"Spoon\", \"Book\"]\n",
        "\n",
        "# Get LabelIDs (used in annotations) for those class names\n",
        "target_labels = classes[classes[\"LabelName\"].isin(target_classes)][\"LabelID\"].tolist()\n",
        "\n",
        "# Load annotations\n",
        "annotations = pd.read_csv(\n",
        "    'https://storage.googleapis.com/openimages/2018_04/train/train-annotations-bbox.csv'\n",
        ")\n",
        "\n",
        "# Save Locally\n",
        "annotations.to_csv('train-annotations-bbox.csv', index=False)\n",
        "classes.to_csv('class-descriptions-boxable.csv', index=False)\n",
        "\n",
        "# Filter annotations by selected target labels\n",
        "filtered = annotations[annotations['LabelName'].isin(target_labels)]\n",
        "\n",
        "# Get unique image IDs for those annotations\n",
        "image_ids = filtered['ImageID'].unique()\n",
        "\n",
        "# Save the image IDs with the \"train/\" prefix for downloading later\n",
        "with open('my_image_list.txt', 'w') as f:\n",
        "    for img_id in image_ids:\n",
        "        f.write(f\"train/{img_id}\\n\")"
      ],
      "metadata": {
        "id": "NLKY-gYKM24w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python downloader.py my_image_list.txt --download_folder=custom_images --num_processes=5"
      ],
      "metadata": {
        "id": "M4mVUwxeMGQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All Necessary preprocessing functions"
      ],
      "metadata": {
        "id": "hurlD1hsDEAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np; import matplotlib.pyplot as plt; import cv2\n",
        "def show_img(im, ax=None, figsize=(8,8), title=None):\n",
        "    if not ax: _,ax = plt.subplots(1,1,figsize=figsize)\n",
        "    if len(im.shape)==2: im = np.tile(im[:,:,None], 3)\n",
        "    ax.imshow(im);\n",
        "    ax.xaxis.set_visible(False)\n",
        "    ax.yaxis.set_visible(False)\n",
        "    if title: ax.set_title(title)\n",
        "    return ax\n",
        "\n",
        "def show_imgs(ims, rows=1, figsize=(16,8), title=[None]):\n",
        "    title = title*len(ims) if len(title) == 1 else title\n",
        "    _,ax = plt.subplots(rows, len(ims)//rows, figsize=figsize)\n",
        "    [show_img(im,ax_,title=tit) for im,ax_,tit in zip(ims,ax.flatten(),title)]\n",
        "    return ax\n",
        "\n",
        "def visualize_detections(image_path, detections):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    draw = ImageDraw.Draw(img, mode=\"RGBA\")\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "    for det in detections:\n",
        "        box = det['bbox']\n",
        "        label = det['label']\n",
        "        score = det['score']\n",
        "        for offset in range(-2, 3):\n",
        "            draw.rectangle(\n",
        "                [box[0] + offset, box[1] + offset, box[2] - offset, box[3] - offset],\n",
        "                outline=(0, 255, 0, 255)\n",
        "            )\n",
        "        text = f\"{label} {score:.2f}\"\n",
        "        text_bbox = draw.textbbox((box[0], box[1]), text, font=font)\n",
        "        draw.rectangle(text_bbox, fill=(0, 0, 0, 160))  # Background\n",
        "        draw.text((box[0], box[1]), text, fill='yellow', font=font)  # Foreground text\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "C1PbBMtGC6uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predefined parameters"
      ],
      "metadata": {
        "id": "MzT37byNF-D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coco_labels = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n",
        "    'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
        "    'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "    'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n",
        "    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
        "    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "    'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n",
        "    'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]"
      ],
      "metadata": {
        "id": "toeKONdMF8xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cv2 detect"
      ],
      "metadata": {
        "id": "EMMf34O_JF7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_opencv_object_detection(image_path, known_width_mm=19.05, min_area=3500, draw=True):\n",
        "    img = cv2.imread(image_path)\n",
        "    orig = img.copy()\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    edged = cv2.Canny(blurred, 50, 100)\n",
        "\n",
        "\n",
        "    # Finding contours\n",
        "    cnts, _ = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = sorted(cnts, key=lambda c: cv2.boundingRect(c)[0])\n",
        "\n",
        "    dimensions = {}\n",
        "    pixels_per_mm = None\n",
        "\n",
        "    for i, c in enumerate(cnts):\n",
        "        if cv2.contourArea(c) < min_area:\n",
        "            continue\n",
        "\n",
        "        box = cv2.minAreaRect(c)\n",
        "        (w_px, h_px) = box[1]\n",
        "        box_pts = cv2.boxPoints(box)\n",
        "        box_pts = np.array(box_pts, dtype=\"int\")\n",
        "\n",
        "        if pixels_per_mm is None:\n",
        "            ref_width_pixels = min(w_px, h_px)\n",
        "            pixels_per_mm = ref_width_pixels / known_width_mm\n",
        "            # print(f\"[Reference] Pixels per mm: {pixels_per_mm:.4f}\")\n",
        "            continue\n",
        "\n",
        "        width_mm = w_px / pixels_per_mm\n",
        "        height_mm = h_px / pixels_per_mm\n",
        "        dimensions[i] = (round(width_mm, 2), round(height_mm, 2))\n",
        "\n",
        "        if draw:\n",
        "            cv2.drawContours(img, [box_pts], -1, (0, 255, 0), 2)\n",
        "            center = np.mean(box_pts, axis=0).astype(int)\n",
        "            label = f\"{width_mm:.1f}x{height_mm:.1f}mm\"\n",
        "            cv2.putText(img, label, tuple(center), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n",
        "\n",
        "    if draw:\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(\"OpenCV Contour Detection\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    return dimensions, img\n",
        "run_opencv_object_detection('images/images1.jpg');"
      ],
      "metadata": {
        "id": "lTRDaeI8JEzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "yolo detection"
      ],
      "metadata": {
        "id": "MhUapi2PGfbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "def yolo_detect(image_path, model=yolo_model, score_threshold=0.8):\n",
        "    start_time = time.time()\n",
        "    results = model(image_path)[0]\n",
        "    end_time = time.time()\n",
        "\n",
        "    detections = []\n",
        "    for box in results.boxes:\n",
        "        score = float(box.conf)\n",
        "        if score >= score_threshold:\n",
        "            label_idx = int(box.cls)\n",
        "            label = model.names[label_idx]\n",
        "            bbox = box.xyxy[0].tolist()\n",
        "            detections.append({\n",
        "                'label': label,\n",
        "                'bbox': bbox,\n",
        "                'score': score\n",
        "            })\n",
        "\n",
        "    return detections, image_path, end_time - start_time\n",
        "\n",
        "# Calling the function\n",
        "detections, img, program_time = yolo_detect('images/images4.jpg')\n",
        "print(f\"YOLO Inference Time: {program_time:.2f} seconds\")\n",
        "visualize_detections(img, detections)"
      ],
      "metadata": {
        "id": "T77PQWvCHLuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fastRCNN model detect and visualize"
      ],
      "metadata": {
        "id": "J_bkuSUpHE1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fasterrcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "fasterrcnn_model.eval()\n",
        "def fastrcnn_detect(image_path, model=fasterrcnn_model, score_threshold=0.8):\n",
        "    transform = T.Compose([T.ToTensor()])\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = transform(img)\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        predictions = model([img_tensor])\n",
        "    end_time = time.time()\n",
        "\n",
        "    detections = []\n",
        "    for i, box in enumerate(predictions[0]['boxes']):\n",
        "        label_idx = predictions[0]['labels'][i].item()\n",
        "        label = coco_labels[label_idx]\n",
        "        score = predictions[0]['scores'][i].item()\n",
        "        if score >= score_threshold and label != 'N/A':\n",
        "            detections.append({\n",
        "                'label': label,\n",
        "                'bbox': box.tolist(),\n",
        "                'score': score\n",
        "            })\n",
        "\n",
        "    return detections, image_path, end_time - start_time\n",
        "\n",
        "# Calling the function\n",
        "detections, img, program_time = fastrcnn_detect('images/images4.jpg')\n",
        "\n",
        "print(f'Time for Inference: {program_time:.2f} seconds')\n",
        "visualize_detections(img, detections)"
      ],
      "metadata": {
        "id": "gVDShECrGAa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using FastRCNN pretrained model"
      ],
      "metadata": {
        "id": "pKbV8PxpIfmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# url for downloading the pretrained model ...\n",
        "\n",
        "fasterrcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
        "checkpoint = torch.load(\"faster_rcnn_model.pth\", map_location=torch.device(\"cpu\"))\n",
        "fasterrcnn_model.load_state_dict(checkpoint)\n",
        "fasterrcnn_model.eval()\n",
        "\n",
        "def fastrcnn_detect(image_path, model=fasterrcnn_model, score_threshold=0.8):\n",
        "    transform = T.Compose([T.ToTensor()])\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = transform(img)\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        predictions = model([img_tensor])\n",
        "    end_time = time.time()\n",
        "\n",
        "    detections = []\n",
        "    for i, box in enumerate(predictions[0]['boxes']):\n",
        "        label_idx = predictions[0]['labels'][i].item()\n",
        "        label = coco_labels[label_idx] if label_idx < len(coco_labels) else str(label_idx)\n",
        "        score = predictions[0]['scores'][i].item()\n",
        "        if score >= score_threshold and label != 'N/A':\n",
        "            detections.append({\n",
        "                'label': label,\n",
        "                'bbox': box.tolist(),\n",
        "                'score': score\n",
        "            })\n",
        "\n",
        "    return detections, image_path, end_time - start_time\n",
        "\n",
        "# Usage\n",
        "detections, img, program_time = fastrcnn_detect('images/images4.jpg')\n",
        "print(f'Time for Inference: {program_time:.2f} seconds')\n",
        "\n",
        "# Visualization function call\n",
        "visualize_detections(img, detections)"
      ],
      "metadata": {
        "id": "5F6c0E0aILxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pewpew"
      ],
      "metadata": {
        "id": "IWBSHanMKja9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function to filter atleast a sample of 100 images for training the model\n",
        "\"\"\"\n",
        "target_classes = [\"Plate\", \"Spoon\", \"Book\"]\n",
        "classes = pd.read_csv(\"class-descriptions-boxable.csv\", header=None)\n",
        "target_label_names = classes[classes[1].isin(target_classes)][0].tolist()\n",
        "\n",
        "# Load full annotations\n",
        "full_annots = pd.read_csv(\"train-annotations-bbox.csv\")\n",
        "\n",
        "# Filter annotations for only those target classes\n",
        "filtered_annots = full_annots[full_annots['LabelName'].isin(target_label_names)]\n",
        "\n",
        "# Select the first 100 unique images\n",
        "selected_image_ids = filtered_annots['ImageID'].unique()[:100]\n",
        "\n",
        "subset_annots = filtered_annots[filtered_annots['ImageID'].isin(selected_image_ids)]\n",
        "\n",
        "selected_image_ids = filtered_annots['ImageID'].unique()[:100]\n",
        "subset_annots = filtered_annots[filtered_annots['ImageID'].isin(selected_image_ids)]\n",
        "\n",
        "subset_annots.to_csv(\"subset-annotations.csv\", index=False)\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "source_folder = \"custom_images\"\n",
        "target_folder = \"custom_images_100\"\n",
        "os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "for img_id in selected_image_ids:\n",
        "    src_path = os.path.join(source_folder, f\"{img_id}.jpg\")\n",
        "    dst_path = os.path.join(target_folder, f\"{img_id}.jpg\")\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.copy(src_path, dst_path)"
      ],
      "metadata": {
        "id": "2Ujy9Ub_K6_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset."
      ],
      "metadata": {
        "id": "2xI2IQS5LH_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This class definition to Load the Open Images Dataset.\n",
        "'''\n",
        "class OpenImagesDataset(Dataset):\n",
        "    def __init__(self, root, annotation_csv, classes_csv, target_classes, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.annots = pd.read_csv(annotation_csv)\n",
        "        self.classes = pd.read_csv(classes_csv, header=None)\n",
        "\n",
        "        # Get target label codes\n",
        "        self.target_classes = target_classes\n",
        "        self.label_names = self.classes[self.classes[1].isin(target_classes)][0].tolist()\n",
        "\n",
        "        # Filter only relevant annotations\n",
        "        self.annots = self.annots[self.annots['LabelName'].isin(self.label_names)]\n",
        "        self.image_ids = self.annots['ImageID'].unique()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.image_ids[idx]\n",
        "        img_path = os.path.join(self.root, image_id + '.jpg')\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        image_annots = self.annots[self.annots['ImageID'] == image_id]\n",
        "        for _, row in image_annots.iterrows():\n",
        "            xmin = row['XMin'] * img.width\n",
        "            xmax = row['XMax'] * img.width\n",
        "            ymin = row['YMin'] * img.height\n",
        "            ymax = row['YMax'] * img.height\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(self.label_names.index(row['LabelName']) + 1)\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": torch.tensor([idx])}\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "# Transformations\n",
        "transform = T.Compose([T.ToTensor()])\n",
        "\n",
        "# Target classes\n",
        "target_classes = [\"Plate\", \"Spoon\", \"Book\"]\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = OpenImagesDataset(\n",
        "    root=\"custom_images_100\",\n",
        "    annotation_csv=\"subset-annotations.csv\",\n",
        "    classes_csv=\"class-descriptions-boxable.csv\",\n",
        "    target_classes=target_classes,\n",
        "    transforms=transform\n",
        ")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Load pre-trained Faster R-CNN model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Replace the box predictor\n",
        "num_classes = len(target_classes) + 1\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "vyqkTM_2JbZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training the model"
      ],
      "metadata": {
        "id": "y5sfypXaLFyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm, trange\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, targets in tqdm(data_loader, desc=f\"Training Epoch {epoch+1}\", leave=False):\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += losses.item()\n",
        "\n",
        "    avg_loss = running_loss / len(data_loader)\n",
        "    print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "QjRwmNqQK_t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model"
      ],
      "metadata": {
        "id": "C_Nj5kS8LDrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'faster_rcnn_model.pth')"
      ],
      "metadata": {
        "id": "o5NdqAa_LC51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Placing Algorithm"
      ],
      "metadata": {
        "id": "PUOD_cHb2VNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./input/Elhedhli_dataset', exist_ok=True)\n",
        "url = 'https://raw.githubusercontent.com/ocansey11/computer-vision/main/Q4RealBPP_DataGen.py'\n",
        "local_path = './Q4RealBPP_DataGen.py'\n",
        "urllib.request.urlretrieve(url, local_path)\n",
        "\n",
        "# Import dynamically\n",
        "spec = importlib.util.spec_from_file_location(\"Q4RealBPP_DataGen\", local_path)\n",
        "Q4RealBPP_DataGen = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(Q4RealBPP_DataGen)\n",
        "\n",
        "ProductDataset = Q4RealBPP_DataGen.ProductDataset"
      ],
      "metadata": {
        "id": "2WNElfOu2Xnj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pkl_url = 'https://raw.githubusercontent.com/ocansey11/computer-vision/main/Elhedhli_dataset/products.pkl'\n",
        "local_pkl = './products.pkl'\n",
        "urllib.request.urlretrieve(pkl_url, local_pkl)\n",
        "\n",
        "# Load and preprocess\n",
        "df = pd.read_pickle(local_pkl)\n",
        "df = df[['width', 'depth', 'weight']]\n",
        "df.rename(columns={'depth': 'height'}, inplace=True)\n",
        "df['id'] = ['item_' + str(i) for i in range(len(df))]\n"
      ],
      "metadata": {
        "id": "j4jfohzy7nC1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LqrLGCft7owT",
        "outputId": "280e9246-60fc-4a6a-9d62-6a3d9c01a732"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   width  height  weight      id\n",
              "0    284     226      25  item_0\n",
              "1    299     207      90  item_1\n",
              "2    534     372      41  item_2\n",
              "3    295     134      31  item_3\n",
              "4    266     186      54  item_4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4cbc3e9-3e40-4538-b7e5-d129ec9414c1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>284</td>\n",
              "      <td>226</td>\n",
              "      <td>25</td>\n",
              "      <td>item_0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>299</td>\n",
              "      <td>207</td>\n",
              "      <td>90</td>\n",
              "      <td>item_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>534</td>\n",
              "      <td>372</td>\n",
              "      <td>41</td>\n",
              "      <td>item_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>295</td>\n",
              "      <td>134</td>\n",
              "      <td>31</td>\n",
              "      <td>item_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>266</td>\n",
              "      <td>186</td>\n",
              "      <td>54</td>\n",
              "      <td>item_4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4cbc3e9-3e40-4538-b7e5-d129ec9414c1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f4cbc3e9-3e40-4538-b7e5-d129ec9414c1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f4cbc3e9-3e40-4538-b7e5-d129ec9414c1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-039a2b6e-f25b-4ec4-853d-b23b69ca7c37\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-039a2b6e-f25b-4ec4-853d-b23b69ca7c37')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-039a2b6e-f25b-4ec4-853d-b23b69ca7c37 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def generate_2d_item_list(\n",
        "    num_items=10,\n",
        "    bin_width=1000,\n",
        "    bin_height=1000,\n",
        "    min_dim=100,\n",
        "    max_dim=400,\n",
        "    min_weight=1,\n",
        "    max_weight=10,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a synthetic 2D test case of items for packing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_items : int\n",
        "        Number of items to generate.\n",
        "    bin_width, bin_height : int\n",
        "        Dimensions of the container (for reference).\n",
        "    min_dim, max_dim : int\n",
        "        Min and max for width/height of items.\n",
        "    min_weight, max_weight : int\n",
        "        Min and max weight for each item.\n",
        "    seed : int\n",
        "        Random seed for reproducibility.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    container : tuple\n",
        "        (height, width) of the bin/container.\n",
        "    item_list : dict\n",
        "        Mapping from item_id to (height, width) tuple.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    instance = ProductDataset(\n",
        "        products_path='./input/Elhedhli_dataset/products.pkl',\n",
        "        num_categories=num_items,\n",
        "        min_width=min_dim,\n",
        "        max_width=max_dim,\n",
        "        min_depth=min_dim,\n",
        "        max_depth=max_dim,\n",
        "        min_height=0,     # Ignored for 2D\n",
        "        max_height=0,\n",
        "        min_weight=min_weight,\n",
        "        max_weight=max_weight,\n",
        "        force_overload=True\n",
        "    )\n",
        "\n",
        "    df = instance.products\n",
        "    df = df[['width', 'depth']]\n",
        "    df.rename(columns={'depth': 'height'}, inplace=True)\n",
        "    df['id'] = ['item_' + str(i) for i in range(len(df))]\n",
        "\n",
        "    # Convert to item_list format: {item_id: (height, width)}\n",
        "    item_list = {row['id']: (row['height'], row['width']) for _, row in df.iterrows()}\n",
        "\n",
        "    container = (bin_height, bin_width)\n",
        "    return container, item_list\n"
      ],
      "metadata": {
        "id": "y8yI9rHW_2Dm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_2d_solution(container, layout_df):\n",
        "    \"\"\"\n",
        "    Visualize the 2D layout of packed items inside a container.\n",
        "\n",
        "    This function creates a matplotlib plot of the container with each packed item\n",
        "    represented as a colored rectangle. Each item is annotated with its ID.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    container : dict\n",
        "        A dictionary specifying the dimensions of the container. Should contain:\n",
        "            - 'width' : int or float, the container's width.\n",
        "            - 'height': int or float, the container's height.\n",
        "\n",
        "    layout_df : pandas.DataFrame\n",
        "        A DataFrame with the placement information for each item. Must contain columns:\n",
        "            - 'x'      : x-coordinate (left) of the item's position.\n",
        "            - 'y'      : y-coordinate (top) of the item's position.\n",
        "            - 'width'  : width of the item.\n",
        "            - 'height' : height of the item.\n",
        "            - 'id'     : identifier of the item (used for labeling).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Displays a matplotlib plot showing the packing layout.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xlim(0, container['width'])\n",
        "    ax.set_ylim(0, container['height'])\n",
        "    ax.set_title('2D Knapsack Layout')\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "    for _, row in layout_df.iterrows():\n",
        "        rect = patches.Rectangle(\n",
        "            (row['x'], row['y']),\n",
        "            row['width'],\n",
        "            row['height'],\n",
        "            linewidth=1,\n",
        "            edgecolor='black',\n",
        "            facecolor=np.random.rand(3,)\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(row['x'] + 5, row['y'] + 5, row['id'], fontsize=8)\n",
        "\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "XBp2yTd58UoU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_tetris_packing_with_fail(container_size, item_list, grid_unit=20):\n",
        "    \"\"\"\n",
        "    Attempts to pack items into a 2D container using a greedy placement strategy.\n",
        "\n",
        "    This is the core helper function for follow up genetic algorithm for 2d bin placement 2D bin packing. Items are sorted\n",
        "    by width (in grid units) and placed top-left first. Items that cannot be placed due to space or collision constraints are returned separately.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    container_size : tuple of int\n",
        "        The (height, width) of the container in real units (e.g., pixels, cm).\n",
        "\n",
        "    item_list : dict\n",
        "        Dictionary mapping item IDs to their (height, width) in real units.\n",
        "\n",
        "    grid_unit : int, optional\n",
        "        The size of each discrete grid cell. Both container and items are discretized\n",
        "        using this unit to simplify placement logic (default is 20).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    placements : list of dict\n",
        "        Each dict contains:\n",
        "            - 'item'   : item ID\n",
        "            - 'row'    : top Y coordinate (in real units)\n",
        "            - 'col'    : left X coordinate (in real units)\n",
        "            - 'height' : item height (in real units)\n",
        "            - 'width'  : item width (in real units)\n",
        "\n",
        "    unplaced_items : list\n",
        "        A list of item IDs that could not be placed in the container.\n",
        "    \"\"\"\n",
        "    H, W = int(container_size[0] // grid_unit), int(container_size[1] // grid_unit)\n",
        "    grid = [[0 for _ in range(W)] for _ in range(H)]\n",
        "\n",
        "    # Convert dimensions to grid units and sort by width (descending)\n",
        "    items = sorted(\n",
        "        [(i, max(1, round(h / grid_unit)), max(1, round(w / grid_unit)))\n",
        "         for i, (h, w) in item_list.items()],\n",
        "        key=lambda x: x[2], reverse=True\n",
        "    )\n",
        "\n",
        "    placements = []\n",
        "    unplaced_items = []\n",
        "\n",
        "    for item_id, h, w in items:\n",
        "        placed = False\n",
        "        for r in range(H):\n",
        "            for c in range(W):\n",
        "                if can_place(grid, r, c, h, w, H, W):\n",
        "                    place_item(grid, r, c, h, w, item_id)\n",
        "                    placements.append({\n",
        "                        'item': item_id,\n",
        "                        'row': r * grid_unit,\n",
        "                        'col': c * grid_unit,\n",
        "                        'height': h * grid_unit,\n",
        "                        'width': w * grid_unit\n",
        "                    })\n",
        "                    placed = True\n",
        "                    break\n",
        "            if placed:\n",
        "                break\n",
        "        if not placed:\n",
        "            unplaced_items.append(item_id)\n",
        "\n",
        "    return placements, unplaced_items\n",
        "\n",
        "\n",
        "def can_place(grid, r, c, h, w, H, W):\n",
        "    \"\"\"\n",
        "    Checks whether an item of height h and width w can be placed at (r, c) on the grid.\n",
        "\n",
        "    Ensures the item stays within bounds and does not overlap with any existing items.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    grid : list of list of int\n",
        "        2D grid representing container occupancy.\n",
        "    r, c : int\n",
        "        Row and column to attempt placing the item.\n",
        "    h, w : int\n",
        "        Item height and width in grid units.\n",
        "    H, W : int\n",
        "        Total height and width of the grid.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        True if the item can be placed without collisions or overflow.\n",
        "    \"\"\"\n",
        "    if r + h > H or c + w > W:\n",
        "        return False\n",
        "    for i in range(r, r + h):\n",
        "        for j in range(c, c + w):\n",
        "            if grid[i][j] != 0:\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def place_item(grid, r, c, h, w, item_id):\n",
        "    \"\"\"\n",
        "    Places an item on the grid by marking its occupied cells with the item ID.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    grid : list of list of int\n",
        "        2D grid representing container occupancy.\n",
        "    r, c : int\n",
        "        Top-left grid cell where the item will be placed.\n",
        "    h, w : int\n",
        "        Item height and width in grid units.\n",
        "    item_id : int or str\n",
        "        Identifier for the item to mark on the grid.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    for i in range(r, r + h):\n",
        "        for j in range(c, c + w):\n",
        "            grid[i][j] = item_id\n"
      ],
      "metadata": {
        "id": "x0un8Ww98m-G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genetic_algorithm_packing(container_size, item_list, grid_unit=20,\n",
        "                              population_size=10, generations=20, mutation_rate=0.1,\n",
        "                              visualize=True):\n",
        "    \"\"\"\n",
        "    Optimize item packing using a Genetic Algorithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    container_size : tuple\n",
        "        (height, width) of the container.\n",
        "    item_list : dict\n",
        "        Mapping of item_id to (height, width).\n",
        "    grid_unit : int\n",
        "        Grid discretization unit.\n",
        "    population_size : int\n",
        "        Number of candidate solutions per generation.\n",
        "    generations : int\n",
        "        Total number of evolution steps.\n",
        "    mutation_rate : float\n",
        "        Probability of mutation per child.\n",
        "    visualize : bool\n",
        "        If True, plots the best packing layout.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Contains best_individual, best_placements, best_unplaced, best_score.\n",
        "    \"\"\"\n",
        "    def evaluate(individual):\n",
        "        \"\"\"\n",
        "        Evaluate the fitness of an individual permutation.\n",
        "\n",
        "        Returns cost, placements, and list of unplaced items.\n",
        "        \"\"\"\n",
        "        shuffled_items = {i: item_list[i] for i in individual}\n",
        "        placements, unplaced = greedy_tetris_packing_with_fail(container_size, shuffled_items, grid_unit)\n",
        "\n",
        "        total_area = sum([h * w for _, h, w in [(i, max(1, round(item_list[i][0] / grid_unit)),\n",
        "                                                 max(1, round(item_list[i][1] / grid_unit)))\n",
        "                                                for i in item_list]])\n",
        "        used_area = sum([p['height'] * p['width'] for p in placements])\n",
        "        cost = len(unplaced) * 1000 + (total_area - used_area)\n",
        "        return cost, placements, unplaced\n",
        "\n",
        "    item_ids = list(item_list.keys())\n",
        "    population = [random.sample(item_ids, len(item_ids)) for _ in range(population_size)]\n",
        "    best_individual, best_placements, best_unplaced = None, [], []\n",
        "    best_score = float('inf')\n",
        "\n",
        "    print(\"Starting Genetic Algorithm...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for gen in tqdm(range(generations), desc=\"Evolving generations\"):\n",
        "        scored_population = []\n",
        "        for individual in population:\n",
        "            cost, placements, unplaced = evaluate(individual)\n",
        "            scored_population.append((cost, individual, placements, unplaced))\n",
        "            if cost < best_score:\n",
        "                best_score = cost\n",
        "                best_individual = individual\n",
        "                best_placements = placements\n",
        "                best_unplaced = unplaced\n",
        "\n",
        "        scored_population.sort(key=lambda x: x[0])\n",
        "        survivors = scored_population[:population_size // 2]\n",
        "\n",
        "        children = []\n",
        "        while len(children) < population_size - len(survivors):\n",
        "            parent1 = random.choice(survivors)[1]\n",
        "            parent2 = random.choice(survivors)[1]\n",
        "            child = crossover(parent1, parent2)\n",
        "            child = mutate(child, mutation_rate)\n",
        "            children.append(child)\n",
        "\n",
        "        population = [s[1] for s in survivors] + children\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\n Genetic algorithm finished in {end_time - start_time:.2f} seconds.\")\n",
        "    print(f\"Best Cost = {best_score}, Unplaced Items = {len(best_unplaced)}\")\n",
        "\n",
        "    if visualize and best_placements:\n",
        "        layout_df = pd.DataFrame(best_placements).rename(columns={'col': 'x', 'row': 'y'})\n",
        "        layout_df['id'] = layout_df['item'].astype(str)\n",
        "        plot_2d_solution(container={'width': container_size[1], 'height': container_size[0]},\n",
        "                         layout_df=layout_df)\n",
        "\n",
        "    return {\n",
        "        'best_individual': best_individual,\n",
        "        'best_placements': best_placements,\n",
        "        'best_unplaced': best_unplaced,\n",
        "        'best_score': best_score\n",
        "    }\n",
        "\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "    \"\"\"\n",
        "    Order crossover (OX) between two parents.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    parent1, parent2 : list\n",
        "        Two parent permutations.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        New child permutation.\n",
        "    \"\"\"\n",
        "    size = len(parent1)\n",
        "    start, end = sorted(random.sample(range(size), 2))\n",
        "    child = [None] * size\n",
        "    child[start:end] = parent1[start:end]\n",
        "\n",
        "    fill_positions = [item for item in parent2 if item not in child]\n",
        "    pointer = 0\n",
        "    for i in range(size):\n",
        "        if child[i] is None:\n",
        "            child[i] = fill_positions[pointer]\n",
        "            pointer += 1\n",
        "    return child\n",
        "\n",
        "\n",
        "def mutate(individual, mutation_rate):\n",
        "    \"\"\"\n",
        "    Randomly swaps two items in the individual based on mutation rate.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    individual : list\n",
        "        Permutation of item IDs.\n",
        "    mutation_rate : float\n",
        "        Probability of mutation.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        Mutated individual.\n",
        "    \"\"\"\n",
        "    if random.random() < mutation_rate:\n",
        "        i, j = random.sample(range(len(individual)), 2)\n",
        "        individual[i], individual[j] = individual[j], individual[i]\n",
        "    return individual\n"
      ],
      "metadata": {
        "id": "WWys638N-lFX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_tetris_packing_with_rotation(container_size, item_list, grid_unit=20):\n",
        "    H, W = int(container_size[0] // grid_unit), int(container_size[1] // grid_unit)\n",
        "    grid = [[0 for _ in range(W)] for _ in range(H)]\n",
        "\n",
        "    items = sorted(\n",
        "        [(i, max(1, round(h / grid_unit)), max(1, round(w / grid_unit)))\n",
        "         for i, (h, w) in item_list.items()],\n",
        "        key=lambda x: x[2], reverse=True\n",
        "    )\n",
        "\n",
        "    placements = []\n",
        "    unplaced_items = []\n",
        "\n",
        "    for item_id, h, w in items:\n",
        "        placed = False\n",
        "        orientations = [(h, w, False), (w, h, True)]  # Try both orientations\n",
        "        for height, width, rotated in orientations:\n",
        "            for r in range(H):\n",
        "                for c in range(W):\n",
        "                    if can_place(grid, r, c, height, width, H, W):\n",
        "                        place_item(grid, r, c, height, width, item_id)\n",
        "                        placements.append({\n",
        "                            'item': item_id,\n",
        "                            'row': r * grid_unit,\n",
        "                            'col': c * grid_unit,\n",
        "                            'height': height * grid_unit,\n",
        "                            'width': width * grid_unit,\n",
        "                            'rotated': rotated\n",
        "                        })\n",
        "                        placed = True\n",
        "                        break\n",
        "                if placed:\n",
        "                    break\n",
        "            if placed:\n",
        "                break\n",
        "        if not placed:\n",
        "            unplaced_items.append(item_id)\n",
        "\n",
        "    return placements, unplaced_items\n"
      ],
      "metadata": {
        "id": "Rbk4Kx_EDcP8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def simulated_annealing_packing(container_size, item_list, grid_unit=20, initial_temp=1000, cooling_rate=0.95, max_iter=1000, visualize=True, track_progress=False):\n",
        "    item_ids = list(item_list.keys())\n",
        "    current_order = random.sample(item_ids, len(item_ids))\n",
        "\n",
        "    current_placements, current_unplaced = greedy_tetris_packing_with_rotation(container_size, {i: item_list[i] for i in current_order}, grid_unit)\n",
        "    current_cost = compute_cost(current_placements, current_unplaced, item_list, grid_unit)\n",
        "\n",
        "    best_order = current_order[:]\n",
        "    best_cost = current_cost\n",
        "    best_placements = current_placements\n",
        "    best_unplaced = current_unplaced\n",
        "\n",
        "    temp = initial_temp\n",
        "    cost_trace = []\n",
        "\n",
        "    print(\"Starting Simulated Annealing...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for step in tqdm(range(max_iter), desc=\"Annealing steps\"):\n",
        "        neighbor_order = current_order[:]\n",
        "        i, j = random.sample(range(len(neighbor_order)), 2)\n",
        "        neighbor_order[i], neighbor_order[j] = neighbor_order[j], neighbor_order[i]\n",
        "\n",
        "        neighbor_placements, neighbor_unplaced = greedy_tetris_packing_with_rotation(container_size, {i: item_list[i] for i in neighbor_order}, grid_unit)\n",
        "        neighbor_cost = compute_cost(neighbor_placements, neighbor_unplaced, item_list, grid_unit)\n",
        "\n",
        "        delta = neighbor_cost - current_cost\n",
        "\n",
        "        if delta < 0 or random.random() < math.exp(-delta / temp):\n",
        "            current_order = neighbor_order\n",
        "            current_cost = neighbor_cost\n",
        "            current_placements = neighbor_placements\n",
        "            current_unplaced = neighbor_unplaced\n",
        "\n",
        "            if current_cost < best_cost:\n",
        "                best_order = current_order[:]\n",
        "                best_cost = current_cost\n",
        "                best_placements = current_placements\n",
        "                best_unplaced = current_unplaced\n",
        "\n",
        "        temp *= cooling_rate\n",
        "        if track_progress:\n",
        "            cost_trace.append(best_cost)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\n Simulated Annealing finished in {end_time - start_time:.2f} seconds.\")\n",
        "    print(f\"Best Cost = {best_cost}, Unplaced Items = {len(best_unplaced)}\")\n",
        "\n",
        "    # Optional plot of layout\n",
        "    if visualize and best_placements:\n",
        "        layout_df = pd.DataFrame(best_placements).rename(columns={'col': 'x', 'row': 'y'})\n",
        "        layout_df['id'] = layout_df['item'].astype(str)\n",
        "        plot_2d_solution(\n",
        "            container={'width': container_size[1], 'height': container_size[0]},\n",
        "            layout_df=layout_df\n",
        "        )\n",
        "\n",
        "\n",
        "    return {\n",
        "        'best_order': best_order,\n",
        "        'best_placements': best_placements,\n",
        "        'best_unplaced': best_unplaced,\n",
        "        'best_cost': best_cost,\n",
        "        'cost_trace': cost_trace if track_progress else None\n",
        "    }\n"
      ],
      "metadata": {
        "id": "sUN-EAZADmHc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating our test Case Using benchmark Dataset"
      ],
      "metadata": {
        "id": "wpH2Jtwe_OZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches\n",
        "\n",
        "\n",
        "container, item_df = generate_2d_test_case(num_items=20)\n",
        "item_list = {\n",
        "    i: (int(row['height']), int(row['width']))\n",
        "    for i, (_, row) in enumerate(item_df.iterrows())\n",
        "}\n",
        "\n",
        "\n",
        "ga_result = genetic_algorithm_packing(container_size=(1000, 1000), item_list=item_list, grid_unit=10, population_size=100, generations=15, mutation_rate=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "lZMpnpNq-_X-",
        "outputId": "8efdf55c-2482-4246-df91-2e9bb164866d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Genetic Algorithm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evolving generations: 100%|██████████| 15/15 [01:15<00:00,  5.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Genetic algorithm finished in 75.34 seconds.\n",
            "Best Cost = -781311, Unplaced Items = 10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ga_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8t6vQxKEJGv",
        "outputId": "a92b60d3-4c2c-4207-a4c2-1e6961b5a751"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_individual': [17,\n",
              "  10,\n",
              "  8,\n",
              "  4,\n",
              "  6,\n",
              "  7,\n",
              "  0,\n",
              "  1,\n",
              "  11,\n",
              "  18,\n",
              "  2,\n",
              "  9,\n",
              "  14,\n",
              "  19,\n",
              "  3,\n",
              "  15,\n",
              "  13,\n",
              "  5,\n",
              "  12,\n",
              "  16],\n",
              " 'best_placements': [{'item': 17,\n",
              "   'row': 0,\n",
              "   'col': 0,\n",
              "   'height': 210,\n",
              "   'width': 400},\n",
              "  {'item': 10, 'row': 0, 'col': 400, 'height': 190, 'width': 400},\n",
              "  {'item': 4, 'row': 190, 'col': 400, 'height': 250, 'width': 400},\n",
              "  {'item': 6, 'row': 210, 'col': 0, 'height': 260, 'width': 400},\n",
              "  {'item': 7, 'row': 440, 'col': 400, 'height': 190, 'width': 400},\n",
              "  {'item': 1, 'row': 470, 'col': 0, 'height': 200, 'width': 400},\n",
              "  {'item': 11, 'row': 630, 'col': 400, 'height': 190, 'width': 400},\n",
              "  {'item': 2, 'row': 670, 'col': 0, 'height': 290, 'width': 400},\n",
              "  {'item': 18, 'row': 820, 'col': 400, 'height': 160, 'width': 280},\n",
              "  {'item': 14, 'row': 820, 'col': 680, 'height': 180, 'width': 280}],\n",
              " 'best_unplaced': [15, 13, 0, 5, 3, 8, 9, 19, 16, 12],\n",
              " 'best_score': -781311}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa_result = simulated_annealing_packing(\n",
        "    container_size=(1000, 1000),\n",
        "    item_list=item_list,\n",
        "    grid_unit=30,\n",
        "    initial_temp=500,\n",
        "    cooling_rate=0.98,\n",
        "    max_iter=500,\n",
        "    track_progress=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "zxrdzvumDvcY",
        "outputId": "84b51581-6986-421c-bbbf-8d8763760be2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Simulated Annealing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Annealing steps: 100%|██████████| 500/500 [00:04<00:00, 119.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Simulated Annealing finished in 4.20 seconds.\n",
            "Best Cost = -1055960, Unplaced Items = 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sa_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IuKXUwFEHFW",
        "outputId": "25e0dee1-9c65-4e48-81ea-c577a8eb6094"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_order': [2,\n",
              "  15,\n",
              "  11,\n",
              "  4,\n",
              "  7,\n",
              "  8,\n",
              "  12,\n",
              "  9,\n",
              "  16,\n",
              "  0,\n",
              "  1,\n",
              "  14,\n",
              "  6,\n",
              "  17,\n",
              "  13,\n",
              "  18,\n",
              "  5,\n",
              "  10,\n",
              "  19,\n",
              "  3],\n",
              " 'best_placements': [{'item': 2,\n",
              "   'row': 0,\n",
              "   'col': 0,\n",
              "   'height': 300,\n",
              "   'width': 390,\n",
              "   'rotated': False},\n",
              "  {'item': 15,\n",
              "   'row': 0,\n",
              "   'col': 390,\n",
              "   'height': 180,\n",
              "   'width': 390,\n",
              "   'rotated': False},\n",
              "  {'item': 11,\n",
              "   'row': 180,\n",
              "   'col': 390,\n",
              "   'height': 180,\n",
              "   'width': 390,\n",
              "   'rotated': False},\n",
              "  {'item': 4,\n",
              "   'row': 300,\n",
              "   'col': 0,\n",
              "   'height': 240,\n",
              "   'width': 390,\n",
              "   'rotated': False},\n",
              "  {'item': 7,\n",
              "   'row': 360,\n",
              "   'col': 390,\n",
              "   'height': 180,\n",
              "   'width': 390,\n",
              "   'rotated': False},\n",
              "  {'item': 0,\n",
              "   'row': 540,\n",
              "   'col': 0,\n",
              "   'height': 300,\n",
              "   'width': 390,\n",
              "   'rotated': False},\n",
              "  {'item': 1,\n",
              "   'row': 540,\n",
              "   'col': 0,\n",
              "   'height': 210,\n",
              "   'width': 390,\n",
              "   'rotated': False},\n",
              "  {'item': 6,\n",
              "   'row': 540,\n",
              "   'col': 390,\n",
              "   'height': 270,\n",
              "   'width': 390,\n",
              "   'rotated': False},\n",
              "  {'item': 17,\n",
              "   'row': 750,\n",
              "   'col': 0,\n",
              "   'height': 210,\n",
              "   'width': 390,\n",
              "   'rotated': False},\n",
              "  {'item': 13,\n",
              "   'row': 0,\n",
              "   'col': 780,\n",
              "   'height': 390,\n",
              "   'width': 210,\n",
              "   'rotated': True},\n",
              "  {'item': 10,\n",
              "   'row': 810,\n",
              "   'col': 390,\n",
              "   'height': 180,\n",
              "   'width': 390,\n",
              "   'rotated': False},\n",
              "  {'item': 16,\n",
              "   'row': 390,\n",
              "   'col': 780,\n",
              "   'height': 270,\n",
              "   'width': 210,\n",
              "   'rotated': True},\n",
              "  {'item': 14,\n",
              "   'row': 660,\n",
              "   'col': 780,\n",
              "   'height': 270,\n",
              "   'width': 180,\n",
              "   'rotated': True}],\n",
              " 'best_unplaced': [5, 3, 8, 9, 19, 18, 12],\n",
              " 'best_cost': -1055960,\n",
              " 'cost_trace': [-935360,\n",
              "  -935360,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1029960,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1039760,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960,\n",
              "  -1055960]}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Appendix: Preprocessing Steps <a id=\"appendix-preprocessing\"></a>\n"
      ],
      "metadata": {
        "id": "jD2-lGSHJ8BK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. Hertrich, C. and Skutella, M., 2023. *Provably Good Solutions to the Knapsack Problem via Neural Networks of Bounded Size*. INFORMS Journal on Computing, 35(5), pp.1079–1097.\n",
        "\n",
        "2. Plotkin, A. V., 2022. *Fast algorithm for quadratic knapsack problem*. Vestnik of Saint Petersburg University. Mathematics. Mechanics. Astronomy, 9(1), pp.76–84.\n",
        "\n",
        "3. Fomeni, F. D. and Letchford, A. N., 2014. *A Dynamic Programming Heuristic for the Quadratic Knapsack Problem*. INFORMS Journal on Computing, 26(1), pp.173–182. Available from: https://dx.doi.org/10.1287/ijoc.2013.0555.\n",
        "\n",
        "4. Le, A. V. et al., 2018. *Complete Path Planning for a Tetris-Inspired Self-Reconfigurable Robot by the Genetic Algorithm of the Traveling Salesman Problem*. Electronics, 7(12), p.344. Available from: https://dx.doi.org/10.3390/electronics7120344.\n",
        "\n",
        "5. Tan, L. et al., 2021. *Comparison of YOLO v3, Faster R-CNN, and SSD for Real-Time Pill Identification*.\n",
        "\n",
        "6. Hanif, M. Z. et al., 2024. *Rupiah Banknotes Detection Comparison of The Faster R-CNN Algorithm and YOLOv5*. JURNAL INFOTEL, 16(3).\n",
        "\n",
        "7. Vilcapoma, P. et al., 2024. *Comparison of Faster R-CNN, YOLO, and SSD for Third Molar Angle Detection in Dental Panoramic X-rays*. Sensors, 24(18), p.6053.\n",
        "\n",
        "8. Nedashkivskyi, S., 2024. *Using Convolutional Neural Networks for Solving Problems of Object Detection*. Věda a perspektivy, 5(36).\n",
        "\n",
        "9. He, Z. and He, Y., 2025. *AS-Faster-RCNN: An Improved Object Detection Algorithm for Airport Scene Based on Faster R-CNN*. IEEE Access, 13, pp.36050–36064.\n",
        "\n",
        "10. Munapo, E., 2020. *Improvement of the branch and bound algorithm for solving the knapsack linear integer problem*. Eastern-European Journal of Enterprise Technologies, 2(4(104)), pp.59–69.\n",
        "\n",
        "11. Kang, S. et al., 2024. *Theoretical analysis of integer programming models for the two-dimensional two-staged knapsack problem*. Optimization Letters.\n",
        "\n",
        "12. Li, M. et al., 2024. *TA-YOLO: a lightweight small object detection model based on multi-dimensional trans-attention module for remote sensing images*. Complex & Intelligent Systems, 10(4), pp.5459–5473.\n",
        "\n",
        "13. Othman, N. A. et al., 2018. *An Embedded Real-Time Object Detection and Measurement of its Size*. 2018 International Conference on Artificial Intelligence and Data Processing (IDAP), Malatya, Turkey, pp.1–4. doi:10.1109/IDAP.2018.8620812.\n",
        "\n",
        "14. Singh, K. et al., 2023. *Surface Area Calculation of Asymmetric/Axisymmetric Shapes Utilising Simple Image Processing and OpenCV*. 2023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA), Pune, India, pp.1–8. doi:10.1109/ICCUBEA58933.2023.10392058.\n",
        "\n",
        "15. Hanif, M.Z., Saputra, W.A., Choo, Y.H. and Yunus, A.P., 2020. A neural network-based approach to solving the knapsack problem. *IEEE Access*, [online] Available at: <https://ieeexplore.ieee.org/document/9291401> [Accessed 14 May 2025].\n",
        "\n",
        "16. Franco, C., Gonçalves, J., Silva, S. and Machado, P., 2016. How humans solve the knapsack problem: A study using eye-tracking. *Scientific Reports*, 6, p.34851. Available at: <https://www.nature.com/articles/srep34851> [Accessed 14 May 2025].\n",
        "\n",
        "17. Martello, S. and Toth, P., 1990. Knapsack problems: algorithms and computer implementations. Chichester: Wiley.\n",
        "\n",
        "18. Mitchell, M., 1998. An introduction to genetic algorithms. Cambridge, Mass.: MIT Press.\n",
        "\n",
        "19. Silva, D.F., da Silva, J.S. and Miyata, H., 2021. ‘A review of two-dimensional packing problems and solution techniques’. Computers & Industrial Engineering, 154, p.107111.\n",
        "\n",
        "20. Elhedhli, S., Gzara, F. & Yildiz, B., 2019. Three-dimensional bin packing and mixed-case palletization. INFORMS Journal on Optimization, 1(4), pp.323–352.\n",
        "\n",
        "21. TECNALIA, 2023. Q4RealBPP: A Data Generator for Bin Packing Problems [Python script]. Available at: https://github.com/TECNALIA/Q4RealBPP [Accessed 21 04 2025].\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V6LkwCfdmypZ"
      }
    }
  ]
}